{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6f35d0-f6c4-44df-b1d4-9fe6f208dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__ SCv1_En ___\n",
      "__ SS-Twitter_En ___\n",
      "__ SS-Youtube_En ___\n",
      "__ business_En ___\n",
      "__ haha2018_Es ___\n",
      "__ meoffendes2021_mx_Es ___\n",
      "__ mexa3t2018_aggress_Es ___\n",
      "__ misoginia_Es ___\n",
      "__ semeval2018_anger_Es ___\n",
      "dataset, f1_macro, precision_macro, recall_macro, f1_micro\n",
      "SCv1_En, 0.76331, 0.76343, 0.76361, 0.76333\n",
      "SS-Twitter_En, 0.86701, 0.86914, 0.86538, 0.87000\n",
      "SS-Youtube_En, 0.88162, 0.87503, 0.88905, 0.90554\n",
      "business_En, 0.97374, 0.97222, 0.97917, 0.96875\n",
      "haha2018_Es, 0.84910, 0.84889, 0.84932, 0.86293\n",
      "meoffendes2021_mx_Es, 0.79809, 0.78351, 0.81782, 0.85631\n",
      "mexa3t2018_aggress_Es, 0.84828, 0.84515, 0.85168, 0.87038\n",
      "misoginia_Es, 0.88636, 0.88660, 0.88645, 0.88636\n",
      "semeval2018_anger_Es, 0.56955, 0.54097, 0.69082, 0.65394\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "#import winsound\n",
    "#import sklearn.model_selection as model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "#from microtc.utils import save_model\n",
    "\n",
    "fnames = glob(\"datasets//*_train.json\")\n",
    "#fnames = glob(\"datasets/meoffendes2021_mx_Es_train.json\")\n",
    "fnames.sort()\n",
    "\n",
    "P = []\n",
    "for fn in fnames:\n",
    "\n",
    "    ds_n = fn.replace(\"datasets\\\\\",'').replace('_train.json','')\n",
    "    print('__', ds_n, '___')\n",
    "\n",
    "    train_df = pd.read_json(fn, lines=True)\n",
    "    test_df = pd.read_json(fn.replace(\"_train\", \"_test\"), lines=True)\n",
    "\n",
    "    # recortados para pruebas\n",
    "    #train_df = train_df.loc[0:399, ['text', 'klass']]\n",
    "    #test_df = test_df.loc[0:79, ['text', 'klass']]\n",
    "    \n",
    "    if True:\n",
    "        \"\"\"\n",
    "        TODO: en cada append, el dataset se reconstruye completo. Encontrar otro método más rápido.\n",
    "        \"\"\"\n",
    "        _ = train_df.iloc[0]\n",
    "        bi_train_df = pd.DataFrame([_], columns = ['text', 'klass'])\n",
    "        for index, row in train_df.iloc[1:].iterrows():\n",
    "            try:\n",
    "                bi_train_df =  bi_train_df.append(row)\n",
    "                if _['klass'] == row['klass']:\n",
    "                    bi_frase = {'text': _['text'] + ' ' + row['text'], 'klass':row['klass']}\n",
    "                    bi_train_df = bi_train_df.append(bi_frase, ignore_index=True)\n",
    "                    _ =  {'text': '_V_A_C_I_A_', 'klass': None}\n",
    "                else:   \n",
    "                    _ = row\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "                _ = row\n",
    "                continue\n",
    "        train_df = bi_train_df\n",
    "        #winsound.Beep(3000, 900)\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: No estoy usando el dataset test; en su lugar, hago el split del train. No he visto como\n",
    "    convertir el df en la matrix que pide el MNB\n",
    "    \"\"\"\n",
    "    \n",
    "    #X_train = train_df[['text']]\n",
    "    #y_train = train_df[['klass']]\n",
    "    \n",
    "    #X_test = test_df[['text']]\n",
    "    #y_test = test_df[['klass']]\n",
    "    \n",
    "    \n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "    text_counts = cv.fit_transform(train_df['text']) \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_counts, train_df['klass'], test_size=0.25, random_state=5)\n",
    "    \n",
    "    MNB = MultinomialNB()\n",
    "    MNB.fit(X_train, y_train)\n",
    "    \n",
    "    predicted = MNB.predict(X_test)\n",
    "    \n",
    "    f1_score = metrics.f1_score(predicted, y_test, average=\"macro\")\n",
    "    precision_score = metrics.precision_score(predicted, y_test, average=\"macro\")\n",
    "    recall_score = metrics.recall_score(predicted, y_test, average=\"macro\")\n",
    "    f1_score_micro = metrics.recall_score(predicted, y_test, average=\"micro\")\n",
    "    \n",
    "    p = [ds_n, f1_score, precision_score, recall_score, f1_score_micro] \n",
    "    P.append(p)\n",
    "    \n",
    "    #winsound.Beep(3000, 900)\n",
    "    \n",
    "    #break\n",
    "\n",
    "#save_model(P, 'MNB_BiFrases_True.scores')    \n",
    "    \n",
    "#winsound.Beep(2500, 1600) \n",
    "\n",
    "print('dataset,', 'f1_macro,', 'precision_macro,', 'recall_macro,', 'f1_micro')\n",
    "for p in P:\n",
    "    print(\"{0}, {1:1.5f}, {2:1.5f}, {3:1.5f}, {4:1.5f}\".format(p[0], p[1], p[2], p[3], p[4]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a38ee7e1-d4dc-465a-9e8b-224992e1c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meoffendes2021_mx_Es, 0.68582, 0.67092, 0.72917, 0.76877\n"
     ]
    }
   ],
   "source": [
    "for p in P:\n",
    "    print(\"{0}, {1:1.5f}, {2:1.5f}, {3:1.5f}, {4:1.5f}\".format(p[0], p[1], p[2], p[3], p[4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
